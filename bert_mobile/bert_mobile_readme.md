# BERT Mobile Training and Deployment Framework

A comprehensive end-to-end framework for training, fine-tuning, and deploying BERT models optimized for mobile devices with custom vocabularies and complete integration code.

## ğŸš€ Features

- **ğŸ”§ Custom Vocabulary Building**: Create domain-specific vocabularies from your text data
- **ğŸ“± Mobile Optimization**: Convert BERT models for iOS (CoreML) and Android (TensorFlow Lite)
- **ğŸ¯ Fine-tuning Support**: Fine-tune pre-trained BERT models on custom datasets
- **âš¡ Performance Optimization**: Quantization, pruning, and mobile-specific optimizations
- **ğŸ› ï¸ Complete Pipeline**: End-to-end workflow from data to deployment
- **ğŸ“Š Evaluation Tools**: Comprehensive model evaluation and performance benchmarking
- **ğŸ“¦ Deployment Packages**: Ready-to-use integration code for mobile apps

## ğŸ“ Project Structure

```
bert_mobile/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml       # Model architecture settings
â”‚   â”œâ”€â”€ training_config.yaml    # Training hyperparameters
â”‚   â”œâ”€â”€ mobile_config.yaml      # Mobile optimization settings
â”‚   â””â”€â”€ vocab_config.yaml       # Vocabulary building parameters
â”œâ”€â”€ src/                         # Core source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_downloader.py     # Download BERT models from Hugging Face
â”‚   â”œâ”€â”€ vocab_builder.py        # Build custom vocabularies
â”‚   â”œâ”€â”€ data_processor.py       # Data preprocessing and tokenization
â”‚   â”œâ”€â”€ model_trainer.py        # BERT training with mobile optimizations
â”‚   â”œâ”€â”€ mobile_converter.py     # Convert models for mobile deployment
â”‚   â”œâ”€â”€ tokenizer_utils.py      # Tokenizer utilities and analysis
â”‚   â””â”€â”€ deploy.py               # Create deployment packages
â”œâ”€â”€ scripts/                     # Training and deployment scripts
â”‚   â”œâ”€â”€ download_model.py       # Download BERT models
â”‚   â”œâ”€â”€ build_vocabulary.py     # Build custom vocabularies
â”‚   â”œâ”€â”€ prepare_data.py         # Prepare training data
â”‚   â”œâ”€â”€ train_bert.py           # Train BERT from scratch
â”‚   â”œâ”€â”€ fine_tune_bert.py       # Fine-tune pre-trained models
â”‚   â”œâ”€â”€ convert_to_mobile.py    # Convert for mobile deployment
â”‚   â”œâ”€â”€ evaluate_model.py       # Evaluate model performance
â”‚   â””â”€â”€ deploy.py               # Create deployment packages
â”œâ”€â”€ data/                        # Data directories
â”‚   â”œâ”€â”€ raw/                    # Raw text data
â”‚   â”œâ”€â”€ processed/              # Processed training data
â”‚   â”œâ”€â”€ vocabularies/           # Custom vocabularies
â”‚   â””â”€â”€ tokenized/              # Tokenized data
â”œâ”€â”€ models/                      # Model storage
â”‚   â”œâ”€â”€ pretrained/             # Downloaded pre-trained models
â”‚   â”œâ”€â”€ trained/                # Trained models
â”‚   â”œâ”€â”€ fine_tuned/             # Fine-tuned models
â”‚   â””â”€â”€ mobile/                 # Mobile-optimized models
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ vocabulary_analysis.ipynb
â”‚   â”œâ”€â”€ model_comparison.ipynb
â”‚   â””â”€â”€ mobile_optimization.ipynb
â””â”€â”€ deployment/                  # Mobile deployment packages
    â”œâ”€â”€ android/                # Android integration code
    â””â”€â”€ ios/                    # iOS integration code
```

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone <repository-url>
cd bert_mobile
pip install -r requirements.txt
```

### 2. Download a Pre-trained Model

```bash
# Download BERT base model
python scripts/download_model.py --model_name bert-base-uncased --output_dir ./models/pretrained

# List available models
python scripts/download_model.py --list_models

# Get model recommendations
python scripts/download_model.py --recommend mobile
```

### 3. Build Custom Vocabulary (Optional)

```bash
# Build vocabulary from your text data
python scripts/build_vocabulary.py \
    --input_dir ./data/raw \
    --output_dir ./data/vocabularies \
    --vocab_size 25000 \
    --vocab_name custom_vocab \
    --analyze_domain
```

### 4. Prepare Training Data

```bash
# For classification tasks
python scripts/prepare_data.py \
    --input_dir ./data/raw/classification.csv \
    --vocab_path ./data/vocabularies/custom_vocab.txt \
    --output_dir ./data/processed \
    --task_type classification \
    --text_column text \
    --label_column label

# For language modeling
python scripts/prepare_data.py \
    --input_dir ./data/raw/texts/ \
    --vocab_path ./models/pretrained/bert-base-uncased \
    --output_dir ./data/processed \
    --task_type language_modeling
```

### 5. Fine-tune Model

```bash
# Fine-tune with mobile optimizations
python scripts/fine_tune_bert.py \
    --model_path ./models/pretrained/bert-base-uncased \
    --train_data ./data/processed/train.json \
    --val_data ./data/processed/val.json \
    --custom_vocab ./data/vocabularies/custom_vocab.txt \
    --output_dir ./models/fine_tuned \
    --num_epochs 3 \
    --learning_rate 2e-5 \
    --enable_distillation
```

### 6. Convert for Mobile

```bash
# Convert to mobile formats
python scripts/convert_to_mobile.py \
    --model_path ./models/fine_tuned \
    --output_dir ./models/mobile \
    --platforms both \
    --sequence_length 128 \
    --quantize \
    --validate \
    --benchmark
```

### 7. Create Deployment Package

```bash
# Create deployment packages with integration code
python scripts/deploy.py \
    --model_dir ./models/mobile \
    --platform both \
    --output_dir ./deployment \
    --create_archive
```

## ğŸ“± Mobile Integration

### iOS (Swift)

```swift
import BERTMobile

// Initialize BERT model
do {
    let bert = try BERTMobile()
    
    // Classify text
    let (label, confidence) = try bert.classifyText("This is a great product!")
    print("Classification: \(label) (\(confidence * 100)% confidence)")
    
} catch {
    print("Error: \(error)")
}
```

### Android (Kotlin)

```kotlin
class MainActivity : AppCompatActivity() {
    private lateinit var bert: BERTMobile
    
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        
        // Initialize BERT model
        bert = BERTMobile(this)
        
        // Classify text
        lifecycleScope.launch {
            val (label, confidence) = bert.classifyText("This is a great product!")
            println("Classification: $label (${confidence * 100}% confidence)")
        }
    }
    
    override fun onDestroy() {
        super.onDestroy()
        bert.close()
    }
}
```

## ğŸ”§ Advanced Usage

### Custom Training from Scratch

```bash
python scripts/train_bert.py \
    --model_path ./models/pretrained/bert-base-uncased \
    --train_data ./data/processed/train.json \
    --val_data ./data/processed/val.json \
    --output_dir ./models/trained \
    --task_type classification \
    --num_labels 3 \
    --enable_mobile_optimizations
```

### Model Evaluation

```bash
# Evaluate PyTorch model
python scripts/evaluate_model.py \
    --model_path ./models/fine_tuned \
    --test_data ./data/processed/test.json \
    --platform pytorch \
    --detailed

# Benchmark mobile model
python scripts/evaluate_model.py \
    --model_path ./models/mobile/android/bert_mobile.tflite \
    --test_data ./data/processed/test.json \
    --platform android \
    --benchmark
```

### Vocabulary Analysis

```python
from src.tokenizer_utils import BERTTokenizerUtils

# Analyze tokenization
tokenizer_utils = BERTTokenizerUtils('bert-base-uncased')
stats = tokenizer_utils.analyze_tokenization(your_texts)

print(f"Average tokens per text: {stats['avg_tokens_per_text']:.2f}")
print(f"Subword ratio: {stats['subword_ratio']:.2%}")
print(f"OOV ratio: {stats['oov_ratio']:.2%}")
```

## âš™ï¸ Configuration

### Training Configuration (`config/training_config.yaml`)

```yaml
training:
  num_epochs: 3
  batch_size: 16
  learning_rate: 2e-5
  
  # Mobile optimizations
  mobile_training:
    knowledge_distillation: true
    teacher_model: "bert-base-uncased"
    temperature: 4.0
    alpha: 0.7
    
  # LoRA fine-tuning
  lora:
    enabled: true
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
```

### Mobile Configuration (`config/mobile_config.yaml`)

```yaml
mobile:
  ios:
    compute_units: "neural_engine"
    precision: "float16"
    optimization:
      quantize_weights: true
      
  android:
    delegates: ["gpu", "nnapi"]
    optimization:
      quantization: "dynamic"
      
  performance:
    max_latency_ms: 100
    max_memory_mb: 200
    min_accuracy: 0.85
```

## ğŸ“Š Performance Targets

| Platform | Model Size | Inference Time | Memory Usage | Accuracy |
|----------|------------|----------------|--------------|----------|
| iOS      | < 50MB     | < 100ms        | < 200MB      | > 85%    |
| Android  | < 50MB     | < 150ms        | < 250MB      | > 85%    |

## ğŸ› ï¸ Key Components

### Model Downloader
- Downloads BERT models from Hugging Face Hub
- Supports multiple model variants (base, large, distilled)
- Validates model compatibility for mobile deployment
- Provides model recommendations based on use case

### Vocabulary Builder
- Creates custom vocabularies from domain-specific text
- WordPiece tokenization with configurable parameters
- Domain term analysis and statistics
- Vocabulary optimization for mobile deployment

### Mobile Trainer
- Full BERT training with mobile-specific optimizations
- Knowledge distillation from larger teacher models
- Layer freezing and parameter reduction
- LoRA/QLoRA support for efficient fine-tuning

### Mobile Converter
- Converts PyTorch models to CoreML (iOS) and TensorFlow Lite (Android)
- Applies quantization and optimization techniques
- Validates conversion accuracy
- Benchmarks mobile model performance

### Deployment Manager
- Creates complete deployment packages
- Generates native integration code (Swift/Kotlin)
- Includes example applications and documentation
- Provides troubleshooting guides

## ğŸ” Evaluation and Benchmarking

### Model Accuracy
```bash
python scripts/evaluate_model.py \
    --model_path ./models/mobile/ios/BERTMobile.mlmodel \
    --test_data ./data/processed/test.json \
    --platform ios \
    --output ./results/ios_evaluation.json
```

### Performance Benchmarking
```bash
python scripts/evaluate_model.py \
    --model_path ./models/mobile/android/bert_mobile.tflite \
    --platform android \
    --benchmark \
    --output ./results/android_benchmark.json
```

## ğŸ“± Deployment Packages

The deployment packages include:

### iOS Package
- CoreML model file (`.mlmodel`)
- Swift Package Manager integration
- BERTMobile Swift class with tokenizer
- Example SwiftUI application
- Comprehensive documentation
- Xcode project template

### Android Package
- TensorFlow Lite model (`.tflite`)
- Kotlin integration classes
- Example Android application
- Gradle build configuration
- Asset management utilities
- Performance optimization guides

## ğŸš¨ Troubleshooting

### Common Issues

1. **Model Loading Errors**
   - Verify model file paths and permissions
   - Check model format compatibility
   - Ensure sufficient device memory

2. **Poor Performance**
   - Enable hardware acceleration (GPU/Neural Engine)
   - Reduce model size through quantization
   - Optimize sequence length

3. **Accuracy Issues**
   - Validate tokenizer compatibility
   - Check input preprocessing
   - Compare with original PyTorch model

4. **Memory Issues**
   - Use quantized models
   - Implement proper resource cleanup
   - Monitor memory usage during inference

### Performance Optimization Tips

1. **iOS Optimization**
   - Use Neural Engine when available
   - Enable float16 precision
   - Batch similar requests
   - Cache model instance

2. **Android Optimization**
   - Enable GPU delegate
   - Use NNAPI when supported
   - Implement background processing
   - Consider dynamic quantization

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add comprehensive tests
5. Submit a pull request

## ğŸ“š Citation

If you use this framework in your research, please cite:

```bibtex
@software{bert_mobile_framework_2025,
  title={BERT Mobile: Complete Training and Deployment Framework},
  author={Your Name},
  year={2025},
  url={https://github.com/your-org/bert-mobile}
}
```

## ğŸ“ Support

- ğŸ“– Check the documentation in each deployment package
- ğŸ› Report issues on GitHub
- ğŸ’¬ Join our community discussions
- ğŸ“§ Contact: support@your-org.com

---

**Ready to deploy BERT on mobile? Get started with the quick start guide above!** ğŸš€