# internvl/config/training_config.yaml

training:
  # Basic training parameters
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_steps: 500
  
  # LoRA configuration
  lora:
    enabled: true
    r: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    
  # QLoRA configuration
  qlora:
    enabled: false
    bits: 4
    quant_type: "nf4"
    use_double_quant: true
    
  # Data settings
  data:
    max_length: 2048
    image_size: 448
    padding: "max_length"
    truncation: true
    
  # Optimization
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1e-8
    
  scheduler:
    type: "cosine"
    num_cycles: 0.5
    
  # Logging and checkpointing
  logging:
    steps: 10
    eval_steps: 500
    save_steps: 1000
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 3
    metric: "eval_loss"
    mode: "min"

