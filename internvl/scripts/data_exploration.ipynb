# internvl/notebooks/data_exploration.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL Data Exploration\n",
    "\n",
    "This notebook explores the InternVL training data and provides insights into the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data_path = '../data/processed/train.json'\n",
    "\n",
    "if Path(data_path).exists():\n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} training samples\")\n",
    "else:\n",
    "    print(\"No training data found. Please run data preparation first.\")\n",
    "    data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data structure\n",
    "if data:\n",
    "    print(\"Sample data structure:\")\n",
    "    print(json.dumps(data[0], indent=2))\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    # Analyze question lengths\n",
    "    question_lengths = [len(item['question'].split()) for item in data]\n",
    "    answer_lengths = [len(item['answer'].split()) for item in data]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Question length distribution\n",
    "    ax1.hist(question_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_title('Question Length Distribution')\n",
    "    ax1.set_xlabel('Number of Words')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.axvline(np.mean(question_lengths), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(question_lengths):.1f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Answer length distribution\n",
    "    ax2.hist(answer_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Answer Length Distribution')\n",
    "    ax2.set_xlabel('Number of Words')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(np.mean(answer_lengths), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(answer_lengths):.1f}')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Question length stats: Mean={np.mean(question_lengths):.1f}, \"\n",
    "          f\"Min={min(question_lengths)}, Max={max(question_lengths)}\")\n",
    "    print(f\"Answer length stats: Mean={np.mean(answer_lengths):.1f}, \"\n",
    "          f\"Min={min(answer_lengths)}, Max={max(answer_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most common words in questions and answers\n",
    "if data:\n",
    "    all_questions = ' '.join([item['question'] for item in data]).lower()\n",
    "    all_answers = ' '.join([item['answer'] for item in data]).lower()\n",
    "    \n",
    "    # Remove common stop words for better analysis\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'what', 'where', 'when', 'why', 'how'}\n",
    "    \n",
    "    question_words = [word for word in all_questions.split() if word not in stop_words and len(word) > 2]\n",
    "    answer_words = [word for word in all_answers.split() if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    question_counter = Counter(question_words)\n",
    "    answer_counter = Counter(answer_words)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Top question words\n",
    "    top_question_words = question_counter.most_common(10)\n",
    "    words, counts = zip(*top_question_words)\n",
    "    ax1.barh(words, counts)\n",
    "    ax1.set_title('Top 10 Words in Questions')\n",
    "    ax1.set_xlabel('Frequency')\n",
    "    \n",
    "    # Top answer words\n",
    "    top_answer_words = answer_counter.most_common(10)\n",
    "    words, counts = zip(*top_answer_words)\n",
    "    ax2.barh(words, counts)\n",
    "    ax2.set_title('Top 10 Words in Answers')\n",
    "    ax2.set_xlabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze metadata if available\n",
    "if data and 'metadata' in data[0]:\n",
    "    metadata_df = pd.json_normalize([item['metadata'] for item in data])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(metadata_df.columns), figsize=(15, 5))\n",
    "    if len(metadata_df.columns) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, column in enumerate(metadata_df.columns):\n",
    "        value_counts = metadata_df[column].value_counts()\n",
    "        axes[i].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%')\n",
    "        axes[i].set_title(f'{column.replace(\"_\", \" \").title()} Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Metadata summary:\")\n",
    "    for column in metadata_df.columns:\n",
    "        print(f\"\\n{column}:\")\n",
    "        print(metadata_df[column].value_counts())\n",
    "else:\n",
    "    print(\"No metadata found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMU Data Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze IMU data if present\n",
    "if data and 'imu_data' in data[0]:\n",
    "    print(\"IMU data found in dataset!\")\n",
    "    \n",
    "    # Extract IMU data\n",
    "    imu_data = []\n",
    "    for item in data:\n",
    "        if 'imu_data' in item:\n",
    "            imu_entry = {\n",
    "                'accel_x': item['imu_data']['accel'][0],\n",
    "                'accel_y': item['imu_data']['accel'][1],\n",
    "                'accel_z': item['imu_data']['accel'][2],\n",
    "                'gyro_x': item['imu_data']['gyro'][0],\n",
    "                'gyro_y': item['imu_data']['gyro'][1],\n",
    "                'gyro_z': item['imu_data']['gyro'][2]\n",
    "            }\n",
    "            imu_data.append(imu_entry)\n",
    "    \n",
    "    imu_df = pd.DataFrame(imu_data)\n",
    "    \n",
    "    # IMU data statistics\n",
    "    print(\"\\nIMU Data Statistics:\")\n",
    "    print(imu_df.describe())\n",
    "    \n",
    "    # Plot IMU data distributions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    imu_columns = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "    \n",
    "    for i, col in enumerate(imu_columns):\n",
    "        row = i // 3\n",
    "        col_idx = i % 3\n",
    "        \n",
    "        axes[row, col_idx].hist(imu_df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'{col} Distribution')\n",
    "        axes[row, col_idx].set_xlabel('Value')\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate motion intensity\n",
    "    imu_df['accel_magnitude'] = np.sqrt(imu_df['accel_x']**2 + imu_df['accel_y']**2 + imu_df['accel_z']**2)\n",
    "    imu_df['gyro_magnitude'] = np.sqrt(imu_df['gyro_x']**2 + imu_df['gyro_y']**2 + imu_df['gyro_z']**2)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    ax1.scatter(imu_df['accel_magnitude'], imu_df['gyro_magnitude'], alpha=0.6)\n",
    "    ax1.set_xlabel('Acceleration Magnitude')\n",
    "    ax1.set_ylabel('Gyroscope Magnitude')\n",
    "    ax1.set_title('Motion Intensity Scatter Plot')\n",
    "    \n",
    "    # Motion categories\n",
    "    motion_categories = []\n",
    "    for _, row in imu_df.iterrows():\n",
    "        if row['accel_magnitude'] > 12 or row['gyro_magnitude'] > 1.0:\n",
    "            motion_categories.append('High Motion')\n",
    "        elif row['accel_magnitude'] > 10.5 or row['gyro_magnitude'] > 0.5:\n",
    "            motion_categories.append('Medium Motion')\n",
    "        else:\n",
    "            motion_categories.append('Low Motion')\n",
    "    \n",
    "    motion_counts = Counter(motion_categories)\n",
    "    ax2.pie(motion_counts.values(), labels=motion_counts.keys(), autopct='%1.1f%%')\n",
    "    ax2.set_title('Motion Category Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No IMU data found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data:\n",
    "    print(\"=== DATASET SUMMARY ===\")\n",
    "    print(f\"Total samples: {len(data)}\")\n",
    "    print(f\"Average question length: {np.mean([len(item['question'].split()) for item in data]):.1f} words\")\n",
    "    print(f\"Average answer length: {np.mean([len(item['answer'].split()) for item in data]):.1f} words\")\n",
    "    \n",
    "    if 'metadata' in data[0]:\n",
    "        print(\"\\n=== METADATA AVAILABLE ===\")\n",
    "        metadata_keys = data[0]['metadata'].keys()\n",
    "        for key in metadata_keys:\n",
    "            values = [item['metadata'][key] for item in data]\n",
    "            unique_values = set(values)\n",
    "            print(f\"{key}: {len(unique_values)} unique values - {list(unique_values)[:5]}\")\n",
    "    \n",
    "    if 'imu_data' in data[0]:\n",
    "        print(\"\\n=== IMU DATA AVAILABLE ===\")\n",
    "        print(\"IMU sensors: accelerometer (3-axis) + gyroscope (3-axis)\")\n",
    "        print(\"This enables motion-aware vision-language understanding\")\n",
    "    \n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    \n",
    "    # Check for potential issues\n",
    "    question_lengths = [len(item['question'].split()) for item in data]\n",
    "    answer_lengths = [len(item['answer'].split()) for item in data]\n",
    "    \n",
    "    if max(question_lengths) > 100:\n",
    "        print(\"⚠️  Some questions are very long (>100 words). Consider truncation.\")\n",
    "    \n",
    "    if max(answer_lengths) > 200:\n",
    "        print(\"⚠️  Some answers are very long (>200 words). Consider truncation.\")\n",
    "    \n",
    "    if len(data) < 1000:\n",
    "        print(\"⚠️  Dataset is relatively small. Consider data augmentation.\")\n",
    "    \n",
    "    # Image path validation\n",
    "    missing_images = sum(1 for item in data if not Path(item.get('image_path', '')).exists())\n",
    "    if missing_images > 0:\n",
    "        print(f\"⚠️  {missing_images} samples have missing image files.\")\n",
    "    \n",
    "    print(\"\\n✅ Dataset analysis complete!\")\n",
    "    print(\"   - Use this information to configure training parameters\")\n",
    "    print(\"   - Consider the text length limits for tokenization\")\n",
    "    print(\"   - Ensure sufficient data diversity for good generalization\")\n",
    "else:\n",
    "    print(\"No data available for analysis. Please prepare the dataset first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
