# internvl/notebooks/model_analysis.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL Model Analysis\n",
    "\n",
    "This notebook analyzes InternVL model architecture, performance, and provides insights for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from model_downloader import ModelDownloader\n",
    "import time\n",
    "import psutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model downloader\n",
    "downloader = ModelDownloader()\n",
    "\n",
    "# List available models\n",
    "available_models = downloader.list_available_models()\n",
    "print(\"Available InternVL models:\")\n",
    "for model in available_models:\n",
    "    print(f\"  - {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific model (adjust model_path as needed)\n",
    "model_path = \"./models/pretrained/OpenGVLab_InternVL2-2B\"  # Update this path\n",
    "\n",
    "if Path(model_path).exists():\n",
    "    print(\"Analyzing model architecture...\")\n",
    "    \n",
    "    # Get model information\n",
    "    model_info = downloader.get_model_info(model_path)\n",
    "    \n",
    "    print(\"\\n=== MODEL INFORMATION ===\")\n",
    "    for key, value in model_info.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            if 'parameters' in key:\n",
    "                print(f\"{key}: {value:,}\")\n",
    "            elif 'size_mb' in key:\n",
    "                print(f\"{key}: {value:.2f} MB ({value/1024:.2f} GB)\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Check compatibility\n",
    "    compatibility = downloader.check_model_compatibility(model_path)\n",
    "    print(\"\\n=== COMPATIBILITY ===\")\n",
    "    for key, value in compatibility.items():\n",
    "        status = \"✅\" if value else \"❌\"\n",
    "        print(f\"{status} {key}: {value}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")\n",
    "    print(\"Please download a model first using the download script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(model_path).exists():\n",
    "    # Load model configuration\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    print(\"=== ARCHITECTURE DETAILS ===\")\n",
    "    \n",
    "    # Vision encoder details\n",
    "    if hasattr(config, 'vision_config'):\n",
    "        vision_config = config.vision_config\n",
    "        print(\"\\nVision Encoder:\")\n",
    "        print(f\"  - Image size: {getattr(vision_config, 'image_size', 'Unknown')}\")\n",
    "        print(f\"  - Patch size: {getattr(vision_config, 'patch_size', 'Unknown')}\")\n",
    "        print(f\"  - Hidden size: {getattr(vision_config, 'hidden_size', 'Unknown')}\")\n",
    "        print(f\"  - Layers: {getattr(vision_config, 'num_hidden_layers', 'Unknown')}\")\n",
    "        print(f\"  - Attention heads: {getattr(vision_config, 'num_attention_heads', 'Unknown')}\")\n",
    "    \n",
    "    # Language model details\n",
    "    if hasattr(config, 'llm_config'):\n",
    "        llm_config = config.llm_config\n",
    "        print(\"\\nLanguage Model:\")\n",
    "        print(f\"  - Vocab size: {getattr(llm_config, 'vocab_size', 'Unknown')}\")\n",
    "        print(f\"  - Hidden size: {getattr(llm_config, 'hidden_size', 'Unknown')}\")\n",
    "        print(f\"  - Layers: {getattr(llm_config, 'num_hidden_layers', 'Unknown')}\")\n",
    "        print(f\"  - Attention heads: {getattr(llm_config, 'num_attention_heads', 'Unknown')}\")\n",
    "        print(f\"  - Max position: {getattr(llm_config, 'max_position_embeddings', 'Unknown')}\")\n",
    "    \n",
    "    # General config\n",
    "    print(\"\\nGeneral Configuration:\")\n",
    "    print(f\"  - Model type: {getattr(config, 'model_type', 'Unknown')}\")\n",
    "    print(f\"  - Architecture: {getattr(config, 'architectures', 'Unknown')}\")\n",
    "    print(f\"  - Torch dtype: {getattr(config, 'torch_dtype', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model, tokenizer, num_runs=5):\n",
    "    \"\"\"Benchmark model inference performance.\"\"\"\n",
    "    \n",
    "    # Prepare test inputs\n",
    "    test_text = \"What do you see in this image? Please describe the scene in detail.\"\n",
    "    test_image = torch.randn(1, 3, 224, 224)  # Dummy image\n",
    "    \n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(test_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    test_image = test_image.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(2):\n",
    "            try:\n",
    "                _ = model(images=test_image, **inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Model forward pass failed: {e}\")\n",
    "                return None\n",
    "    \n",
    "    # Benchmark runs\n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        # Measure memory before\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            memory_before = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            memory_before = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                _ = model(images=test_image, **inputs)\n",
    "            except Exception as e:\n",
    "                print(f\"Run {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Measure memory after\n",
    "        if torch.cuda.is_available():\n",
    "            memory_after = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            memory_after = psutil.Process().memory_info().rss / 1024**2  # MB\n",
    "        \n",
    "        times.append(end_time - start_time)\n",
    "        memory_usage.append(memory_after - memory_before)\n",
    "        \n",
    "        print(f\"Run {i+1}: {times[-1]:.3f}s, Memory: {memory_usage[-1]:.1f}MB\")\n",
    "    \n",
    "    if times:\n",
    "        return {\n",
    "            'avg_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'avg_memory': np.mean(memory_usage),\n",
    "            'device': str(device)\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmark\n",
    "if Path(model_path).exists():\n",
    "    try:\n",
    "        print(\"Loading model for benchmarking...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_path, \n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== PERFORMANCE BENCHMARK ===\")\n",
    "        results = benchmark_model(model, tokenizer)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\nBenchmark Results ({results['device']}):\")\n",
    "            print(f\"  Average inference time: {results['avg_time']:.3f} ± {results['std_time']:.3f} seconds\")\n",
    "            print(f\"  Average memory usage: {results['avg_memory']:.1f} MB\")\n",
    "            print(f\"  Throughput: {1/results['avg_time']:.2f} samples/second\")\n",
    "            \n",
    "            # Mobile deployment considerations\n",
    "            print(\"\\n=== MOBILE DEPLOYMENT ANALYSIS ===\")\n",
    "            if results['avg_time'] > 2.0:\n",
    "                print(\"⚠️  Inference time > 2s - Consider model optimization for mobile\")\n",
    "            else:\n",
    "                print(\"✅ Inference time acceptable for mobile deployment\")\n",
    "                \n",
    "            if results['avg_memory'] > 1000:\n",
    "                print(\"⚠️  High memory usage - Consider quantization\")\n",
    "            else:\n",
    "                print(\"✅ Memory usage acceptable for mobile devices\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Benchmarking failed: {e}\")\n",
    "        print(\"This might be due to model loading issues or insufficient resources.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model not available for benchmarking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model files and sizes\n",
    "models_dir = Path(\"../models/pretrained\")\n",
    "\n",
    "if models_dir.exists():\n",
    "    model_sizes = []\n",
    "    model_names = []\n",
    "    \n",
    "    for model_dir in models_dir.iterdir():\n",
    "        if model_dir.is_dir():\n",
    "            total_size = 0\n",
    "            file_count = 0\n",
    "            \n",
    "            for file_path in model_dir.rglob('*'):\n",
    "                if file_path.is_file():\n",
    "                    total_size += file_path.stat().st_size\n",
    "                    file_count += 1\n",
    "            \n",
    "            model_names.append(model_dir.name)\n",
    "            model_sizes.append(total_size / (1024**3))  # Convert to GB\n",
    "            \n",
    "            print(f\"{model_dir.name}:\")\n",
    "            print(f\"  Size: {total_size / (1024**3):.2f} GB\")\n",
    "            print(f\"  Files: {file_count}\")\n",
    "    \n",
    "    if model_sizes:\n",
    "        # Visualize model sizes\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(range(len(model_names)), model_sizes)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Size (GB)')\n",
    "        plt.title('Model Size Comparison')\n",
    "        plt.xticks(range(len(model_names)), [name.replace('_', '\\n') for name in model_names], rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, size in zip(bars, model_sizes):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{size:.2f}GB', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Mobile deployment recommendations\n",
    "        print(\"\\n=== MOBILE DEPLOYMENT RECOMMENDATIONS ===\")\n",
    "        for name, size in zip(model_names, model_sizes):\n",
    "            if size < 1.0:\n",
    "                print(f\"✅ {name}: Suitable for mobile ({size:.2f}GB)\")\n",
    "            elif size < 2.0:\n",
    "                print(f\"⚠️  {name}: May work on high-end mobile devices ({size:.2f}GB)\")\n",
    "            else:\n",
    "                print(f\"❌ {name}: Too large for mobile deployment ({size:.2f}GB)\")\n",
    "                \n",
    "else:\n",
    "    print(\"No models found in the pretrained directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== OPTIMIZATION RECOMMENDATIONS ===\")\n",
    "\n",
    "# Device capabilities\n",
    "print(\"\\nCurrent System:\")\n",
    "print(f\"  CPU: {psutil.cpu_count()} cores\")\n",
    "print(f\"  RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"  GPU: {'Available' if torch.cuda.is_available() else 'Not available'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "\n",
    "# General optimization strategies\n",
    "print(\"\\nGeneral Optimization Strategies:\")\n",
    "print(\"1. 🔧 Quantization:\")\n",
    "print(\"   - Use int8 or int4 quantization to reduce model size\")\n",
    "print(\"   - Can reduce size by 2-4x with minimal accuracy loss\")\n",
    "\n",
    "print(\"\\n2. ✂️  Pruning:\")\n",
    "print(\"   - Remove unnecessary parameters and connections\")\n",
    "print(\"   - Can reduce model size by 10-50%\")\n",
    "\n",
    "print(\"\\n3. 📦 Knowledge Distillation:\")\n",
    "print(\"   - Train smaller student model from larger teacher\")\n",
    "print(\"   - Good for creating mobile-optimized versions\")\n",
    "\n",
    "print(\"\\n4. 🏃 Inference Optimization:\")\n",
    "print(\"   - Use TensorFlow Lite for Android\")\n",
    "print(\"   - Use CoreML for iOS\")\n",
    "print(\"   - Enable hardware acceleration (GPU, Neural Engine)\")\n",
    "\n",
    "# Platform-specific recommendations\n",
    "print(\"\\nPlatform-Specific Recommendations:\")\n",
    "print(\"\\n📱 iOS:\")\n",
    "print(\"   - Target iOS 15+ for Neural Engine support\")\n",
    "print(\"   - Use float16 precision for better performance\")\n",
    "print(\"   - Consider model chunking for large models\")\n",
    "\n",
    "print(\"\\n🤖 Android:\")\n",
    "print(\"   - Use NNAPI delegate when available\")\n",
    "print(\"   - Enable GPU delegate for compatible devices\")\n",
    "print(\"   - Consider dynamic batching for variable inputs\")\n",
    "\n",
    "# Training optimization\n",
    "print(\"\\nTraining Optimization:\")\n",
    "print(\"1. Use LoRA/QLoRA for parameter-efficient fine-tuning\")\n",
    "print(\"2. Apply gradient checkpointing to reduce memory usage\")\n",
    "print(\"3. Use mixed precision training (fp16)\")\n",
    "print(\"4. Implement gradient accumulation for larger effective batch sizes\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. 📊 Profile your specific use case with real data\")\n",
    "print(\"2. 🧪 Experiment with different optimization techniques\")\n",
    "print(\"3. 📱 Test on target mobile devices early and often\")\n",
    "print(\"4. 📈 Monitor accuracy vs. performance trade-offs\")\n",
    "print(\"5. 🔄 Iterate based on deployment requirements\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---

# internvl/notebooks/deployment_testing.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InternVL Mobile Deployment Testing\n",
    "\n",
    "This notebook tests the converted mobile models and validates their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from pathlib import Path\n",
    "from mobile_converter import MobileConverter\n",
    "import json\n",
    "\n",
    "# Platform-specific imports\n",
    "try:\n",
    "    import coremltools as ct\n",
    "    COREML_AVAILABLE = True\n",
    "except ImportError:\n",
    "    COREML_AVAILABLE = False\n",
    "    print(\"CoreML not available - iOS testing disabled\")\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available - Android testing disabled\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test parameters\n",
    "models_dir = Path(\"../models/mobile\")\n",
    "test_image_size = 224\n",
    "test_sequence_length = 512\n",
    "num_test_runs = 10\n",
    "\n",
    "# Create test data\n",
    "test_image = np.random.random((1, 3, test_image_size, test_image_size)).astype(np.float32)\n",
    "test_text_tokens = np.random.randint(0, 1000, (1, test_sequence_length)).astype(np.int32)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test text tokens shape: {test_text_tokens.shape}\")\n",
    "print(f\"Number of test runs: {num_test_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iOS CoreML Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_coreml_model(model_path, test_image, test_tokens, num_runs=5):\n",
    "    \"\"\"Test CoreML model performance.\"\"\"\n",
    "    \n",
    "    if not COREML_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load CoreML model\n",
    "        model = ct.models.model.MLModel(model_path)\n",
    "        print(f\"Loaded CoreML model: {model_path}\")\n",
    "        \n",
    "        # Get model info\n",
    "        spec = model.get_spec()\n",
    "        print(f\"Model description: {spec.description}\")\n",
    "        \n",
    "        # Prepare inputs\n",
    "        input_dict = {\n",
    "            'image': test_image,\n",
    "            'input_ids': test_tokens\n",
    "        }\n",
    "        \n",
    "        # Warmup runs\n",
    "        for _ in range(2):\n",
    "            try:\n",
    "                _ = model.predict(input_dict)\n",
    "            except Exception as e:\n",
    "                print(f\"Warmup failed: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            output = model.predict(input_dict)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            print(f\"Run {i+1}: {times[-1]:.3f}s\")\n",
    "        \n",
    "        results = {\n",
    "            'avg_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'outputs': outputs[0],  # Store one output for analysis\n",
    "            'platform': 'iOS (CoreML)'\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CoreML testing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test iOS models\n",
    "ios_results = []\n",
    "ios_models_dir = models_dir / \"ios\"\n",
    "\n",
    "if ios_models_dir.exists() and COREML_AVAILABLE:\n",
    "    print(\"=== iOS CoreML Testing ===\")\n",
    "    \n",
    "    for model_file in ios_models_dir.glob(\"*.mlmodel\"):\n",
    "        print(f\"\\nTesting {model_file.name}...\")\n",
    "        results = test_coreml_model(str(model_file), test_image, test_text_tokens, num_test_runs)\n",
    "        \n",
    "        if results:\n",
    "            results['model_name'] = model_file.name\n",
    "            results['model_size_mb'] = model_file.stat().st_size / (1024 * 1024)\n",
    "            ios_results.append(results)\n",
    "            \n",
    "            print(f\"Average inference time: {results['avg_time']:.3f} ± {results['std_time']:.3f}s\")\n",
    "            print(f\"Model size: {results['model_size_mb']:.2f} MB\")\n",
    "            print(f\"Throughput: {1/results['avg_time']:.2f} FPS\")\n",
    "        \n",
    "else:\n",
    "    print(\"No iOS models found or CoreML not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Android TensorFlow Lite Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tflite_model(model_path, test_image, test_tokens, num_runs=5):\n",
    "    \"\"\"Test TensorFlow Lite model performance.\"\"\"\n",
    "    \n",
    "    if not TF_AVAILABLE:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load TFLite model\n",
    "        interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        # Get input and output details\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        print(f\"Loaded TFLite model: {model_path}\")\n",
    "        print(f\"Input details: {len(input_details)} inputs\")\n",
    "        print(f\"Output details: {len(output_details)} outputs\")\n",
    "        \n",
    "        # Prepare inputs\n",
    "        for i, detail in enumerate(input_details):\n",
    "            if i == 0:  # Image input\n",
    "                interpreter.set_tensor(detail['index'], test_image)\n",
    "            elif i == 1:  # Text input\n",
    "                interpreter.set_tensor(detail['index'], test_tokens)\n",
    "        \n",
    "        # Warmup runs\n",
    "        for _ in range(2):\n",
    "            try:\n",
    "                interpreter.invoke()\n",
    "            except Exception as e:\n",
    "                print(f\"Warmup failed: {e}\")\n",
    "                return None\n",
    "        \n",
    "        # Benchmark runs\n",
    "        times = []\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            interpreter.invoke()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Get outputs\n",
    "            output = {}\n",
    "            for detail in output_details:\n",
    "                output[detail['name']] = interpreter.get_tensor(detail['index'])\n",
    "            \n",
    "            times.append(end_time - start_time)\n",
    "            outputs.append(output)\n",
    "            \n",
    "            print(f\"Run {i+1}: {times[-1]:.3f}s\")\n",
    "        \n",
    "        results = {\n",
    "            'avg_time': np.mean(times),\n",
    "            'std_time': np.std(times),\n",
    "            'min_time': np.min(times),\n",
    "            'max_time': np.max(times),\n",
    "            'outputs': outputs[0],  # Store one output for analysis\n",
    "            'platform': 'Android (TFLite)'\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"TFLite testing failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test Android models\n",
    "android_results = []\n",
    "android_models_dir = models_dir / \"android\"\n",
    "\n",
    "if android_models_dir.exists() and TF_AVAILABLE:\n",
    "    print(\"=== Android TensorFlow Lite Testing ===\")\n",
    "    \n",
    "    for model_file in android_models_dir.glob(\"*.tflite\"):\n",
    "        print(f\"\\nTesting {model_file.name}...\")\n",
    "        results = test_tflite_model(str(model_file), test_image, test_text_tokens, num_test_runs)\n",
    "        \n",
    "        if results:\n",
    "            results['model_name'] = model_file.name\n",
    "            results['model_size_mb'] = model_file.stat().st_size / (1024 * 1024)\n",
    "            android_results.append(results)\n",
    "            \n",
    "            print(f\"Average inference time: {results['avg_time']:.3f} ± {results['std_time']:.3f}s\")\n",
    "            print(f\"Model size: {results['model_size_mb']:.2f} MB\")\n",
    "            print(f\"Throughput: {1/results['avg_time']:.2f} FPS\")\n",
    "        \n",
    "else:\n",
    "    print(\"No Android models found or TensorFlow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results for comparison\n",
    "all_results = ios_results + android_results\n",
    "\n",
    "if all_results:\n",
    "    print(\"=== PERFORMANCE COMPARISON ===\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    print(f\"{'Platform':<20} {'Model':<20} {'Avg Time (s)':<15} {'Size (MB)':<12} {'FPS':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in all_results:\n",
    "        fps = 1 / result['avg_time']\n",
    "        print(f\"{result['platform']:<20} {result['model_name']:<20} \"\n",
    "              f\"{result['avg_time']:.3f} ± {result['std_time']:.3f}  \"\n",
    "              f\"{result['model_size_mb']:<12.2f} {fps:<8.2f}\")\n",
    "    \n",
    "    # Visualize performance\n",
    "    if len(all_results) > 1:\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        platforms = [r['platform'] for r in all_results]\n",
    "        model_names = [r['model_name'] for r in all_results]\n",
    "        avg_times = [r['avg_time'] for r in all_results]\n",
    "        model_sizes = [r['model_size_mb'] for r in all_results]\n",
    "        fps_values = [1/t for t in avg_times]\n",
    "        \n",
    "        # Inference time comparison\n",
    "        bars1 = ax1.bar(range(len(model_names)), avg_times)\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('Inference Time (s)')\n",
    "        ax1.set_title('Inference Time Comparison')\n",
    "        ax1.set_xticks(range(len(model_names)))\n",
    "        ax1.set_xticklabels([f\"{p}\\n{m}\" for p, m in zip(platforms, model_names)], rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, time_val in zip(bars1, avg_times):\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{time_val:.3f}s', ha='center', va='bottom')\n",
    "        \n",
    "        # Model size comparison\n",
    "        bars2 = ax2.bar(range(len(model_names)), model_sizes)\n",
    "        ax2.set_xlabel('Models')\n",
    "        ax2.set_ylabel('Model Size (MB)')\n",
    "        ax2.set_title('Model Size Comparison')\n",
    "        ax2.set_xticks(range(len(model_names)))\n",
    "        ax2.set_xticklabels([f\"{p}\\n{m}\" for p, m in zip(platforms, model_names)], rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, size in zip(bars2, model_sizes):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                    f'{size:.1f}MB', ha='center', va='bottom')\n",
    "        \n",
    "        # FPS comparison\n",
    "        bars3 = ax3.bar(range(len(model_names)), fps_values)\n",
    "        ax3.set_xlabel('Models')\n",
    "        ax3.set_ylabel('Frames Per Second')\n",
    "        ax3.set_title('Throughput Comparison')\n",
    "        ax3.set_xticks(range(len(model_names)))\n",
    "        ax3.set_xticklabels([f\"{p}\\n{m}\" for p, m in zip(platforms, model_names)], rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, fps in zip(bars3, fps_values):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{fps:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Performance vs Size scatter\n",
    "        colors = ['red' if 'iOS' in p else 'blue' for p in platforms]\n",
    "        ax4.scatter(model_sizes, fps_values, c=colors, s=100, alpha=0.7)\n",
    "        ax4.set_xlabel('Model Size (MB)')\n",
    "        ax4.set_ylabel('Frames Per Second')\n",
    "        ax4.set_title('Performance vs Size Trade-off')\n",
    "        \n",
    "        # Add labels\n",
    "        for i, (size, fps, name) in enumerate(zip(model_sizes, fps_values, model_names)):\n",
    "            ax4.annotate(name, (size, fps), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        # Add legend\n",
    "        ax4.scatter([], [], c='red', label='iOS (CoreML)', alpha=0.7)\n",
    "        ax4.scatter([], [], c='blue', label='Android (TFLite)', alpha=0.7)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"No mobile models found for testing.\")\n",
    "    print(\"Please convert models first using the conversion script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile Deployment Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    print(\"=== MOBILE DEPLOYMENT READINESS ===\")\n",
    "    \n",
    "    for result in all_results:\n",
    "        print(f\"\\n{result['platform']} - {result['model_name']}:\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if result['avg_time'] < 0.5:\n",
    "            perf_status = \"✅ Excellent\"\n",
    "        elif result['avg_time'] < 1.0:\n",
    "            perf_status = \"✅ Good\"\n",
    "        elif result['avg_time'] < 2.0:\n",
    "            perf_status = \"⚠️  Acceptable\"\n",
    "        else:\n",
    "            perf_status = \"❌ Too slow\"\n",
    "        \n",
    "        print(f\"  Performance: {perf_status} ({result['avg_time']:.3f}s avg)\")\n",
    "        \n",
    "        # Size assessment\n",
    "        if result['model_size_mb'] < 50:\n",
    "            size_status = \"✅ Excellent\"\n",
    "        elif result['model_size_mb'] < 100:\n",
    "            size_status = \"✅ Good\"\n",
    "        elif result['model_size_mb'] < 200:\n",
    "            size_status = \"⚠️  Large\"\n",
    "        else:\n",
    "            size_status = \"❌ Too large\"\n",
    "        \n",
    "        print(f\"  Size: {size_status} ({result['model_size_mb']:.1f}MB)\")\n",
    "        \n",
    "        # FPS assessment\n",
    "        fps = 1 / result['avg_time']\n",
    "        if fps > 10:\n",
    "            fps_status = \"✅ Real-time capable\"\n",
    "        elif fps > 5
